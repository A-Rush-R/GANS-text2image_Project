{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "urgfiWQ29uRG"
      },
      "outputs": [],
      "source": [
        "!pip install visualkeras\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import visualkeras\n",
        "from PIL import ImageFont\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Do not forget to connect to GPU runtime before training**"
      ],
      "metadata": {
        "id": "h-EBYRkzcmvV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#importing the dataset\n",
        "(X_train,Y_train),(X_test,Y_test) = tf.keras.datasets.cifar10.load_data()\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train,X_test,Y_train,Y_test=train_test_split(X_train,Y_train,test_size=0.3)"
      ],
      "metadata": {
        "id": "7x_d1W4m-PRp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "id": "QFXXCof1Rbwf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizing the values between -1 and 1\n",
        "\n",
        "X_train  = X_train/255\n",
        "X_test = X_test/255"
      ],
      "metadata": {
        "id": "tFbLM_wBDWRJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an ImageDataGenerator object with given augmentation settings(just an instance)\n",
        "\n",
        "datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "    rotation_range=20,      # Random rotation within the range of [-20, 20] degrees\n",
        "    width_shift_range=0.1,  # Random horizontal shift within the range of [-0.1, 0.1] of the total width\n",
        "    height_shift_range=0.1, # Random vertical shift within the range of [-0.1, 0.1] of the total height\n",
        "    shear_range=0.2,        # Random shearing transformations within the range of [-0.2, 0.2]\n",
        "    zoom_range=0.2,         # Random zoom within the range of [0.8, 1.2]\n",
        "    horizontal_flip=True,   # Randomly flip inputs horizontally\n",
        "    fill_mode='nearest' ,    # Fill any newly created pixels with the nearest available pixel value\n",
        "    validation_split=0.2  # Split 20% of the data for validation\n",
        ")\n",
        "\n",
        "# Apply data augmentation to the training data\n",
        "augmented_images = datagen.flow(X_train, Y_train)\n",
        "\n",
        "# creating the validation data\n",
        "validation_data = datagen.flow(X_train, Y_train, subset='validation')"
        import random
        import matplotlib.pyplot as plt

         figure = plt.figure(figsize=(6, 6))
         for i in range(9):
             index = random.randint(0, len(X_train) - 1)  # Selecting a random index
             ax = figure.add_subplot(3, 3, i + 1)
             ax.imshow(X_train[index])
             ax.set_title(Y_train[index])
             ax.axis('off')

         plt.show()
         import random
         import tensorflow as tf
         import matplotlib.pyplot as plt

         ind = random.randint(0, len(X_train) - 1)
         img = tf.expand_dims(X_train[ind], axis=0)
         aug_iterator = datagen.flow(img, batch_size=1)

         figure = plt.figure(figsize=(6, 6))
         for i in range(9):
            batch = aug_iterator.next()
            img = batch[0].astype('float32')
            ax = figure.add_subplot(3, 3, i + 1)
            ax.imshow(img)
            ax.axis('off')

         plt.show()
      ],
      "metadata": {
        "id": "f5cd4XTEBHtB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **`This is an Open assignment with minimum instructions`**\n",
        "You are allowed to search all over the web--> find any articles or implement them--> try your experiments\n",
        "\n",
        "> **---> create the model**\\\n",
        "**---> tune the hyperparameters like learning_rate, filter/kernel size**\\\n",
        "**---> optimize the result**\n"
      ],
      "metadata": {
        "id": "QvBuEuh4c7V6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "You have got some experience form last assignment '\n",
        "Use that experience this time\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "J9Bqkf61fGz4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title **IMPORTANT ANNOUNCEMENT**\n",
        "\"\"\"\n",
        "Now with this much freedom, you can do anything\n",
        "So make sure you understand what you do and after the end of this assignment\n",
        " you will have explain all the code you tried in a viva exam\n",
        " this will be the mid term evaluation.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "dYfJtQTAdyLh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **YOUR EFFORTS WILL COUNT MORE THE RESULTS YOU GET**\n",
        "> **So make sure all the time you spent on this notebook should be visible from the notebook**"
      ],
      "metadata": {
        "id": "8dJ4qpyjek7L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Question: What is Batch Normalization? Why is it used for? How does it fix the\n",
        "problem it is used for? [ Answer in atleast 300 words]\n",
        "<cite your sources>\n",
        "\n",
        "\n",
        "Answer:\n",
        "Batch normalization is a technique used in deep neural networks to normalize the inputs of each  layer by adjusting and scaling the activations.\n",
        "It helps address the problem of internal covariate shift, which refers to the change in the distribution of input values to a layer as the\n",
        "parameters of the preceding layers change during training.\n",
        "The internal covariate shift can occur in deep networks due to the changing distributions of input values, which makes it difficult for\n",
        "subsequent layers to learn effectively.\n",
        "This phenomenon slows down the training process and requires careful tuning of learning rates to ensure convergence.\n",
        "Batch normalization addresses this problem by normalizing the inputs to a layer.\n",
        "It operates on a batch of inputs and computes the mean and variance of the batch.\n",
        "These statistics are then used to normalize the inputs to have zero mean and unit variance.\n",
        "The normalized inputs are then scaled and shifted using learnable parameters (gamma and beta) to introduce non-linearity and\n",
        "ensure that the layer has the capacity to represent a wide range of input distributions.\n",
        "\n",
        "Batch normalization offers several benefits:\n",
        "\n",
        "1) Improved training speed: By normalizing the inputs, batch normalization reduces the internal covariate shift,\n",
        "allowing for faster and more stable convergence during training. It enables the use of higher learning rates,\n",
        "reducing the number of iterations required to reach convergence.\n",
        "\n",
        "2) Reduced sensitivity to initialization: Batch normalization makes the network less sensitive to the choice of initial weights.\n",
        "It helps alleviate the vanishing/exploding gradient problem by ensuring that the inputs to each layer are within a reasonable range.\n",
        "\n",
        "3) Regularization effect: Batch normalization introduces a slight regularization effect by adding noise to the inputs during training.\n",
        "This noise acts as a form of regularization and helps prevent overfitting.\n",
        "\n",
        "4) Increased network capacity: Batch normalization allows the network to learn more complex and deeper representations.\n",
        "By reducing the internal covariate shift, it enables the use of deeper networks without sacrificing performance.\n",
        "\n",
        "5) Invariance to changes in input scale and distribution: Batch normalization makes the network more robust to changes in input scale and distribution.\n",
        "It allows the network to handle inputs with different ranges and distributions without the need for manual feature scaling.\n",
        "\n",
        "6) Batch normalization has become a standard technique in deep learning and has shown significant improvements in training deep neural networks across various tasks,\n",
        "including image classification, object detection, and natural language processing\n",
        "\n",
        "\n",
        "SOURCE: https://proceedings.mlr.press/v37/ioffe15.html\n",
               : https://youtu.be/2AscwXePInA
        "\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "E92b0edzffZn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Tutorial: https://www.tensorflow.org/tutorials/images/classification\n",
        "\n",
        "Above tutorial does exactly the same job\n",
        "But I will zero marks for exact same model used in the tutorial\n",
        "\n",
        "You need experiment with different layers and all those\n",
        "experiments should be visible by your notebooks\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "EQppujL4gyZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model initialization\n",
        "model = tf.keras.Sequential()\n",
        
        # model initialization
        model = tf.keras.Sequential()
        import tensorflow as tf
        model.add(tf.keras.layers.Conv2D(32, (3, 3), padding='same', input_shape=(32, 32, 3)))
        model.add(tf.keras.layers.BatchNormalization())
        model.add(tf.keras.layers.Activation('relu'))
        model.add(tf.keras.layers.Conv2D(32, (3, 3), padding='same'))
        model.add(tf.keras.layers.BatchNormalization())
        model.add(tf.keras.layers.Activation('relu'))
        model.add(tf.keras.layers.MaxPooling2D((2, 2)))

        model.add(tf.keras.layers.Conv2D(64, (3, 3), padding='valid'))
        model.add(tf.keras.layers.BatchNormalization())
        model.add(tf.keras.layers.Activation('relu'))
        model.add(tf.keras.layers.Conv2D(64, (3, 3), padding='valid'))
        model.add(tf.keras.layers.BatchNormalization())
        model.add(tf.keras.layers.Activation('relu'))
        model.add(tf.keras.layers.MaxPooling2D((2, 2)))

        model.add(tf.keras.layers.Conv2D(128, (3, 3), padding='valid'))
        model.add(tf.keras.layers.BatchNormalization())
        model.add(tf.keras.layers.Activation('relu'))
        model.add(tf.keras.layers.Conv2D(128, (3, 3), padding='valid'))
        model.add(tf.keras.layers.BatchNormalization())
        model.add(tf.keras.layers.Activation('relu'))

        model.add(tf.keras.layers.Flatten())
        model.add(tf.keras.layers.Dense(512))
        model.add(tf.keras.layers.BatchNormalization())
        model.add(tf.keras.layers.Activation('relu'))
        model.add(tf.keras.layers.Dropout(0.5))
        model.add(tf.keras.layers.Dense(128))
        model.add(tf.keras.layers.BatchNormalization())
        model.add(tf.keras.layers.Activation('relu'))
        model.add(tf.keras.layers.Dropout(0.3))





        
        
        
        "\n",
        "# Intermediate layers\n",

        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# final layer\n",
        "model.add(tf.keras.layers.Dense(10, activation='softmax'))"
      ],
      "metadata": {
        "id": "R6NlKl-bB74H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "R5hokZZ1FaDI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Visualization\n",
        "\n",
        "# just run this cell as it is\n",
        "tf.keras.utils.plot_model(model, to_file='cnn_plot.png', show_shapes=True, show_layer_names=True)"
      ],
      "metadata": {
        "id": "UdIOuHAyEEUp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# just run this cell as it is\n",
        "visualkeras.layered_view(model, legend=True)"
      ],
      "metadata": {
        "id": "zllpup2ZEW1Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "In the last part of the assignment\n",
        "try experimenting with learning rate.\n",
        "May be decreasing the lr might had help?\n",
        "\"\"\"\n",
        "\n",
        "model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "                  loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "                  metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "OnEXCG4gEv4Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This is another way of dealing with the generated data\n",
        "# both X_train and Y_train are inside the augmented image\n",
        "\n",
        "history = model.fit(augmented_images, epochs=10,validation_data = validation_data)"
      ],
      "metadata": {
        "id": "KiNN6SYMFBgI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['accuracy'],label=\"Train accuracy\")\n",
        "plt.plot(history.history['val_accuracy'], label = \"Validation accuracy\")\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "Q9QQWplIFJUI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(X_test,Y_test)"
      ],
      "metadata": {
        "id": "6kZ4-pgwfZHn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **TRY DIFFERENT MODELS AND COMPARE THE RESULTS**"
      ],
      "metadata": {
        "id": "u4sAtMyYeWT9"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HPEDcHoWeVOr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
